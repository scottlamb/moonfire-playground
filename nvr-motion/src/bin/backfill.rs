//! Runs video analytics over the entire corpus.
//! Currently doesn't actually do anything with them; just getting the workflow down.
//! TODO: keep state.

use cstr::*;
use failure::{Error, bail};
use log::{info, trace};
use moonfire_ffmpeg::avutil::VideoFrame;
use std::convert::TryFrom;
use std::sync::Arc;
use structopt::StructOpt;
use uuid::Uuid;

#[derive(StructOpt)]
struct Opt {
    #[structopt(short, long, parse(try_from_str))]
    cookie: Option<reqwest::header::HeaderValue>,

    #[structopt(short, long, parse(try_from_str))]
    nvr: reqwest::Url,

    #[structopt(short, long, parse(try_from_str))]
    start: Option<moonfire_nvr_client::Time>,

    #[structopt(short, long, parse(try_from_str))]
    end: Option<moonfire_nvr_client::Time>,
}

struct Context<'a> {
    client: moonfire_nvr_client::Client,
    interpreter: parking_lot::Mutex<moonfire_tflite::Interpreter<'a>>,
    width: usize,
    height: usize,
    start: Option<moonfire_nvr_client::Time>,
    end: Option<moonfire_nvr_client::Time>,
}

/// Gets the id range of committed recordings indicated by `r`.
fn id_range(r: &moonfire_nvr_client::Recording) -> std::ops::Range<i32> {
    let end_id = r.first_uncommitted.unwrap_or(r.end_id.unwrap_or(r.start_id) + 1);
    r.start_id .. end_id
}

async fn process_camera(ctx: &Context<'_>, camera: &moonfire_nvr_client::Camera)
                        -> Result<Option<(Stream, Vec<i32>)>, Error> {
    const DESIRED_STREAM: &str = "sub";
    if !camera.streams.contains_key(DESIRED_STREAM) {
        return Ok(None);
    }
    let recordings = ctx.client.list_recordings(&moonfire_nvr_client::ListRecordingsRequest {
        camera: camera.uuid,
        stream: DESIRED_STREAM,
        start: ctx.start,
        end: ctx.end,
    }).await?;

    let num_recordings = recordings.recordings.iter().map(|r| {
        let range = id_range(r);
        usize::try_from(range.end - range.start).unwrap()
    }).sum();
    let mut ids = Vec::with_capacity(num_recordings);
    for r in &recordings.recordings {
        for id in id_range(r) {
            ids.push(id);
        }
    }
    let stream = Stream {
        camera_short_name: camera.short_name.clone(),
        camera_uuid: camera.uuid,
        stream_name: DESIRED_STREAM.to_owned(),
    };
    Ok(Some((stream, ids)))
}

struct Stream {
    camera_short_name: String,
    camera_uuid: Uuid,
    stream_name: String,
}

async fn process_recording(ctx: &Context<'_>, stream: &Stream, id: i32)
                           -> Result<(), Error> {
    trace!("recording {}/{}/{}", &stream.camera_short_name, &stream.stream_name, id);
    let resp = ctx.client.view(&moonfire_nvr_client::ViewRequest {
        camera: stream.camera_uuid,
        mp4_type: moonfire_nvr_client::Mp4Type::Normal,
        stream: &stream.stream_name,
        s: &id.to_string(),
        ts: false,
    }).await?;
    let body = resp.bytes().await?;

    let mut open_options = moonfire_ffmpeg::avutil::Dictionary::new();
    let mut io_ctx = moonfire_ffmpeg::avformat::SliceIoContext::new(&body);
    let mut input = moonfire_ffmpeg::avformat::InputFormatContext::with_io_context(
        cstr!(""), &mut io_ctx, &mut open_options).unwrap();
    input.find_stream_info().unwrap();

    // In .mp4 files generated by Moonfire NVR, the video is always stream 0.
    // The timestamp subtitles (if any) are stream 1.
    const VIDEO_STREAM: usize = 0;

    let stream = input.streams().get(VIDEO_STREAM);
    let par = stream.codecpar();
    let mut dopt = moonfire_ffmpeg::avutil::Dictionary::new();
    dopt.set(cstr!("refcounted_frames"), cstr!("0")).unwrap();  // TODO?
    let d = par.new_decoder(&mut dopt).unwrap();

    let mut scaled = VideoFrame::owned(moonfire_ffmpeg::avutil::ImageDimensions {
        width: i32::try_from(ctx.width).unwrap(),
        height: i32::try_from(ctx.height).unwrap(),
        pix_fmt: moonfire_ffmpeg::avutil::PixelFormat::rgb24(),
    }).unwrap();
    let mut f = VideoFrame::empty().unwrap();
    let mut s = moonfire_ffmpeg::swscale::Scaler::new(par.dims(), scaled.dims()).unwrap();
    loop {
        let pkt = match input.read_frame() {
            Ok(p) => p,
            Err(e) if e.is_eof() => { break; },
            Err(e) => panic!(e),
        };
        if pkt.stream_index() != VIDEO_STREAM {
            continue;
        }
        if !d.decode_video(&pkt, &mut f).unwrap() {
            continue;
        }
        s.scale(&f, &mut scaled);
        let mut interpreter = ctx.interpreter.lock();
        moonfire_motion::copy(&scaled, &mut interpreter.inputs()[0]);
        interpreter.invoke().unwrap();
    }
    Ok(())
}

#[tokio::main]
async fn main() -> Result<(), Error> {
    let mut h = moonfire_motion::init_logging();
    let _a = h.r#async();
    let opt = Opt::from_args();

    info!("Loading model");
    let m = moonfire_tflite::Model::from_static(moonfire_motion::MODEL).unwrap();
    info!("Creating delegate");
    let devices = moonfire_tflite::edgetpu::Devices::list();
    if devices.is_empty() {
        bail!("no edge tpu ready");
    }
    let delegate = devices[0].create_delegate().unwrap();
    info!("Creating interpreter");
    let mut builder = moonfire_tflite::Interpreter::builder();
    builder.add_delegate(&delegate);
    let mut interpreter = builder.build(&m).unwrap();
    info!("Done creating interpreter");

    let (width, height);
    {
        let inputs = interpreter.inputs();
        let input = &inputs[0];
        let num_dims = input.num_dims();
        assert_eq!(num_dims, 4);
        assert_eq!(input.dim(0), 1);
        height = input.dim(1);
        width = input.dim(2);
        assert_eq!(input.dim(3), 3);
    }

    let ctx = Context {
        client: moonfire_nvr_client::Client::new(opt.nvr, opt.cookie),
        interpreter: parking_lot::Mutex::new(interpreter),
        width,
        height,
        start: opt.start,
        end: opt.end,
    };

    let _ffmpeg = moonfire_ffmpeg::Ffmpeg::new();

    info!("Finding recordings");
    let top_level = ctx.client.top_level(&moonfire_nvr_client::TopLevelRequest::default()).await?;
    let stuff = futures::future::try_join_all(
        top_level.cameras.iter().map(|c| process_camera(&ctx, c))).await?;
    //let streams = Vec::new();
    //let recordings = Vec::new();

    let count = stuff.iter().map(|o| o.as_ref().map(|(_, ids)| ids.len() as u64).unwrap_or(0)).sum();
    info!("Found {} recordings", count);

    let progress = Arc::new(indicatif::ProgressBar::new(count)
        .with_style(indicatif::ProgressStyle::default_bar()
            .template("[{eta_precise}] {bar:40.cyan/blue} {pos:>7}/{len:7} {msg}")
            .progress_chars("##-")));

    for s in &stuff {
        if let Some((stream, ids)) = s {
            for &id in ids {
                process_recording(&ctx, stream, id).await?;
                progress.inc(1);
            }
        }
    }
    progress.finish();
    Ok(())
}
