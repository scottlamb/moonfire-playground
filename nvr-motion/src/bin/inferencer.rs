//! Writes a WebVTT metadata caption file representing all of the objects detected in the given
//! .mp4 file. Each cue represents a single object for a single frame.

//use cstr::*;
use proto::inferencer_client::InferencerClient;
use serde::Serialize;
use std::convert::TryFrom;
use std::env;
use std::ffi::CString;
use std::io::Write;

type BoxedError = Box<dyn std::error::Error + 'static>;

const SCORE_THRESHOLD: f32 = 0.5;

mod proto {
    tonic::include_proto!("org.moonfire_nvr.inferencer");
}

/// Copies from a RGB24 VideoFrame to a 1xHxWx3 Tensor.
fn extract(from: &moonfire_ffmpeg::VideoFrame) -> Vec<u8> {
    let from = from.plane(0);
    let (w, h) = (from.width, from.height);
    let mut to = vec![0; w * h * 3];
    let mut from_i = 0;
    let mut to_i = 0;
    for _y in 0..h {
        to[to_i..to_i+3*w].copy_from_slice(&from.data[from_i..from_i+3*w]);
        from_i += from.linesize;
        to_i += 3*w;
    }
    to
}

#[derive(Serialize)]
struct Object<'m> {
    label: &'m str,
    score: f32,
    x: f32,
    y: f32,
    w: f32,
    h: f32,
}

struct Pts(i64, moonfire_ffmpeg::AVRational);

impl std::fmt::Display for Pts {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        // https://www.w3.org/TR/webvtt1/#webvtt-timestamp
        let seconds = self.0 as f64 * self.1.num as f64 / self.1.den as f64;
        let minutes = (seconds / 60.).trunc();
        let seconds = seconds % 60.;
        let hours = (minutes / 60.).trunc();
        let minutes = minutes % 60.;
        write!(f, "{:02.0}:{:02.0}:{:06.3}", hours, minutes, seconds)
    }
}

fn write_objs(mut stdout: &mut dyn Write, start: Pts, end: Pts, objs: &[Object])
              -> std::io::Result<()> {
    for o in objs {
        write!(stdout, "{} --> {}\n", start, end)?;
        serde_json::to_writer(&mut stdout, o)?;
        write!(stdout, "\n\n")?;
    }
    if !objs.is_empty() {
        stdout.flush()?;
    }
    Ok(())
}

async fn get_model(client: &mut InferencerClient<tonic::transport::Channel>)
                   -> Result<proto::Model, BoxedError> {
    let req = tonic::Request::new(proto::ListModelsRequest {});
    let mut resp = client.list_models(req).await?.into_inner();
    if resp.model.len() != 1 {
        return Err(Box::new(tonic::Status::new(tonic::Code::Internal,
                                               "expected exactly one model")))?;
    }
    Ok(resp.model.remove(0))
}

#[tokio::main]
async fn main() -> Result<(), BoxedError> {
    let mut client = InferencerClient::connect("http://192.168.8.3:8085").await?;

    let model = get_model(&mut client).await?;

    let url = env::args().nth(1).expect("missing url");
    let _ffmpeg = moonfire_ffmpeg::Ffmpeg::new();
    let mut open_options = moonfire_ffmpeg::Dictionary::new();
    let mut input = moonfire_ffmpeg::InputFormatContext::open(&CString::new(url).unwrap(),
                                                              &mut open_options).unwrap();
    input.find_stream_info().unwrap();

    // In .mp4 files generated by Moonfire NVR, the video is always stream 0.
    // The timestamp subtitles (if any) are stream 1.
    const VIDEO_STREAM: usize = 0;

    let stream = input.streams().get(VIDEO_STREAM);
    let time_base = stream.time_base();
    let par = stream.codecpar();
    let mut dopt = moonfire_ffmpeg::Dictionary::new();
    //dopt.set(cstr!("refcounted_frames"), cstr!("0")).unwrap();  // TODO?
    let d = par.new_decoder(&mut dopt).unwrap();

    let model_par = match model.input_parameters.as_ref() {
        None => panic!("model must return input parameters"),
        Some(p) => p,
    };

    let mut scaled = moonfire_ffmpeg::VideoFrame::owned(moonfire_ffmpeg::ImageDimensions {
        width: i32::try_from(model_par.width).unwrap(),
        height: i32::try_from(model_par.height).unwrap(),
        pix_fmt: if model_par.pixel_format == (proto::PixelFormat::Rgb24 as i32) {
            moonfire_ffmpeg::PixelFormat::rgb24()
        } else {
            panic!("Unknown pixel format {}", model_par.pixel_format);
        },
    }).unwrap();
    let mut f = moonfire_ffmpeg::VideoFrame::empty().unwrap();
    let mut s = moonfire_ffmpeg::Scaler::new(par.dims(), scaled.dims()).unwrap();
    let mut prev_pts = 0;
    let mut prev_objs: Vec<Object> = Vec::new();
    let stdout = std::io::stdout();
    let mut stdout = stdout.lock();
    write!(&mut stdout, "WEBVTT\n\n").unwrap();
    loop {
        let pkt = match input.read_frame() {
            Ok(p) => p,
            Err(e) if e.is_eof() => { break; },
            Err(e) => panic!(e),
        };
        if pkt.stream_index() != VIDEO_STREAM {
            continue;
        }
        if !d.decode_video(&pkt, &mut f).unwrap() {
            continue;
        }
        write_objs(&mut stdout, Pts(prev_pts, time_base), Pts(f.pts(), time_base),
                   &prev_objs).unwrap();
        prev_objs.clear();
        prev_pts = f.pts();
        s.scale(&f, &mut scaled);
        let image = extract(&scaled);
        let response = client.process_image(proto::ProcessImageRequest {
            priority: 0,
            model_uuid: model.uuid.clone(),
            image,
        }).await.unwrap();
        let response = response.into_inner();
        let result = response.result.unwrap().model_result.unwrap();
        let result = match result {
            proto::image_result::ModelResult::ObjectDetectionResult(r) => r,
        };
        for i in 0..result.score.len() {
            if result.score[i] < SCORE_THRESHOLD {
                continue;
            }
            let label = match model.labels.get(&result.label[i]) {
                None => continue,
                Some(l) => l.as_str(),
            };
            prev_objs.push(Object {
                y: result.y[i],
                x: result.x[i],
                h: result.h[i],
                w: result.w[i],
                label,
                score: result.score[i],
            });
        }
    }
    write_objs(&mut stdout, Pts(prev_pts, time_base), Pts(stream.duration(), time_base),
               &prev_objs).unwrap();
    Ok(())
}
